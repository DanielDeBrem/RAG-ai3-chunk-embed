# DOELARCHITECTUUR - AI-3 en AI-4 Role Clarity

## ğŸ¯ Kernprincipe

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI-3 = RETRIEVAL ENGINE (parsing, chunking, embeddings)       â”‚
â”‚  AI-4 = INTELLIGENCE LAYER (chat, answers, data extraction)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ… AI-3 Responsibilities (Ingestion Factory)

### Wat AI-3 DOET:
1. **Document Parsing**
   - PDF extractie (native + OCR)
   - Text extraction
   - Structure detection

2. **Chunking**
   - 5 strategieÃ«n beschikbaar
   - Auto-detection van beste strategie
   - Table-aware, page-aware, semantic chunking

3. **Embeddings**
   - BAAI/bge-m3 model (1024-dim)
   - Contextual enrichment met LLM
   - GPU-optimized (GPU 0)

4. **Indexing**
   - FAISS vector store
   - Multi-tenant isolation
   - Deduplication

5. **Retrieval (RAG Search)**
   - Vector similarity search
   - Reranking (BAAI/bge-reranker-v2-m3, GPU 1)
   - Top-k chunk retrieval

### Wat AI-3 NIET DOET:
- âŒ **Final answer generation**
- âŒ **Chat interfaces**
- âŒ **Data extraction voor business logic**
- âŒ **User-facing Q&A**

### Uitzondering:
- âš ï¸ **Fallback mode**: Alleen voor lokaal testen als AI-4 down is
- âš ï¸ **Testing**: Pipeline validatie en kwaliteitstesten

---

## âœ… AI-4 Responsibilities (Intelligence Layer)

### Wat AI-4 DOET:
1. **Chat Interface**
   - User conversations
   - Session management
   - UI/UX

2. **Final Answer Generation**
   - **ALLE antwoorden met llama3.1:70b**
   - Context building van AI-3 chunks
   - Prompt engineering

3. **Data Extraction**
   - Structured data uit documenten
   - Business logic processing
   - Workflow orchestration

4. **Integration**
   - Calls naar AI-3 voor retrieval
   - Context management
   - Error handling

### Wat AI-4 NIET DOET:
- âŒ Document parsing (laat AI-3 doen)
- âŒ Embeddings berekenen (laat AI-3 doen)
- âŒ Vector search (laat AI-3 doen)

---

## ğŸ”„ Typische Flow

### Document Upload
```
User uploads PDF
    â†“
AI-4 ontvangt file
    â†“
AI-4 â†’ POST /ingest (AI-3)
    â†“
AI-3: parse, chunk, embed, index
    â†“
AI-3 â†’ returns: {chunks_added: N}
    â†“
AI-4 toont user: "âœ“ N chunks geÃ¯ndexeerd"
```

### User Query
```
User stelt vraag
    â†“
AI-4 ontvangt query
    â†“
AI-4 â†’ POST /search (AI-3)
    â†“
AI-3: vector search + reranking
    â†“
AI-3 â†’ returns: [{chunk, score}, ...]
    â†“
AI-4: build context van chunks
    â†“
AI-4: generate answer met 70B
    â†“
AI-4 toont answer aan user
```

**KRITIEK:** AI-3 geeft ALLEEN chunks terug, AI-4 genereert ALTIJD het antwoord!

---

## ğŸ“‹ API Endpoints

### AI-3 Endpoints (Port 9000)

**Production:**
- `POST /v1/rag/ingest/text` - Text ingestion
- `POST /v1/rag/ingest/file` - File upload ingestion
- `POST /v1/rag/search` - **Vector search + reranking (RETRIEVAL ONLY)**
- `POST /analyze` - Document analysis (port 9100)
- `GET /health` - Health check

**Deprecated/Testing Only:**
- âš ï¸ `/generate_answer` - Niet voor productie (fallback/test only)
- âš ï¸ Any answer generation endpoints - Route naar AI-4

### AI-4 Endpoints (Port 8000)

**Production:**
- `POST /llm70/generate` - **Final answer generation**
- `POST /llm70/extract` - Data extraction
- `POST /chat` - Chat interface
- Andere business logic endpoints

---

## ğŸš¨ Kritieke Regels

### Regel 1: Scheiding van Verantwoordelijkheden
```
AI-3 = RETRIEVAL
AI-4 = INTELLIGENCE
```

### Regel 2: Geen Final Answers op AI-3
```python
# âŒ FOUT - AI-3 genereert answer
def search_endpoint():
    chunks = find_chunks(query)
    answer = llm.generate(chunks)  # â† FOUT!
    return answer

# âœ… GOED - AI-3 geeft alleen chunks
def search_endpoint():
    chunks = find_chunks(query)
    return chunks  # â† AI-4 doet de rest
```

### Regel 3: Alle Answers via AI-4
```python
# AI-4 code
def handle_user_query(query):
    # 1. Search AI-3
    chunks = ai3_client.search(query)
    
    # 2. Build context
    context = build_context(chunks)
    
    # 3. Generate answer (70B on AI-4)
    answer = llm70.generate(context, query)
    
    # 4. Return to user
    return answer
```

### Regel 4: Fallback Mode
```python
# AI-3 fallback (alleen voor lokaal testen)
def search_with_fallback(query):
    chunks = find_chunks(query)
    
    if AI4_AVAILABLE:
        return chunks  # â† Production flow
    else:
        # âš ï¸ Fallback: Heuristic answer (geen LLM)
        return simple_answer(chunks)
```

---

## ğŸ¨ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    AI-4      â”‚
                  â”‚  (Port 8000) â”‚
                  â”‚              â”‚
                  â”‚ - Chat UI    â”‚
                  â”‚ - 70B LLM    â”‚
                  â”‚ - Business   â”‚
                  â”‚   Logic      â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚              â”‚              â”‚
          â–¼              â–¼              â–¼
    POST /ingest   POST /search   POST /analyze
          â”‚              â”‚              â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    AI-3      â”‚
                  â”‚  (Port 9000) â”‚
                  â”‚              â”‚
                  â”‚ - Parsing    â”‚
                  â”‚ - Chunking   â”‚
                  â”‚ - Embeddings â”‚
                  â”‚ - Indexing   â”‚
                  â”‚ - Search     â”‚
                  â”‚ - Reranking  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
              Returns: chunks with scores
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    AI-4      â”‚
                  â”‚ Generate     â”‚
                  â”‚ Answer (70B) â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    USER      â”‚
                  â”‚  (Answer)    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Waarom Deze Scheiding?

### Voordelen:

1. **Specialization**
   - AI-3: Geoptimaliseerd voor high-throughput retrieval (8 GPU's)
   - AI-4: Geoptimaliseerd voor intelligence (70B model)

2. **Scalability**
   - AI-3 kan meerdere AI-4 instances bedienen
   - Load balancing mogelijk

3. **Maintainability**
   - Clear responsibilities
   - Makkelijker te debuggen
   - Independent updates

4. **Cost Efficiency**
   - 70B model draait alleen op AI-4 (1 server)
   - Retrieval gedistribueerd (AI-3 multi-GPU)

5. **Flexibility**
   - AI-4 kan verschillende LLM's gebruiken
   - AI-3 blijft stabiel als retrieval engine

---

## ğŸ“š Gerelateerde Documentatie

- `ARCHITECTURE.md` - Volledige architectuur details
- `AI4_INTEGRATION_GUIDE.md` - Integratie instructies voor AI-4
- `DATAFACTORY_API_SPEC.md` - API specificaties
- `CHUNKING_STRATEGIES_README.md` - Chunking strategieÃ«n

---

## âœ… Checklist voor Implementatie

### AI-3 (Ingestion Factory)
- [x] Document parsing met OCR support
- [x] 5 chunking strategieÃ«n
- [x] Embedding service (GPU 0)
- [x] FAISS indexing
- [x] Vector search endpoint
- [x] Reranker service (GPU 1)
- [ ] Verwijder answer generation endpoints (behalve fallback)
- [ ] Update documentation

### AI-4 (Intelligence Layer)
- [ ] DataFactory client implementeren
- [ ] Document upload flow naar AI-3
- [ ] Search flow naar AI-3
- [ ] Answer generation met 70B
- [ ] Chat interface
- [ ] Error handling
- [ ] Health monitoring

---

## ğŸ¯ Success Criteria

De architectuur is succesvol wanneer:

1. âœ… AI-3 genereert **GEEN** final answers (behalve fallback)
2. âœ… AI-4 genereert **ALLE** final answers met 70B
3. âœ… Flow: User â†’ AI-4 â†’ AI-3 (search) â†’ AI-4 (answer) â†’ User
4. âœ… AI-3 API returnt alleen chunks, niet answers
5. âœ… Clear separation of concerns
6. âœ… Documentation up to date

---

**Laatste update:** 12 januari 2026

*Deze architectuur is de doelstaat voor het RAG systeem. AI-3 en AI-4 werken samen, elk met hun eigen verantwoordelijkheid.*
